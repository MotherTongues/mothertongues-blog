
var documents = [{
    "id": 0,
    "url": "https://blog.mothertongues.org/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://blog.mothertongues.org/about",
    "title": "",
    "body": "Welome to the Mother Tongues Blog. "
    }, {
    "id": 2,
    "url": "https://blog.mothertongues.org/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://blog.mothertongues.org/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                             Welcome                              :               Welcome to the Mother Tongues official blog!:                                                                                                                                                                       Aidan                                17 Mar 2020                                                                                                                            All Stories:                                     Preventing feeding of g2p rules with case              :       When you write g2p mappings, sometimes you find yourself wanting to prevent feeding between rules, but your situation is too complex to be handled by the normal prevent-feeding option. :                                                                               Eric Joanis                                07 Dec 2021                                                                    G2P Part 7: Contributing a new mapping to g2p for everyone to use              :       This is the last part of the seven-part series on g2p. In this part, we’ll discuss how to contribute your mappings to the main g2p library. :                                                                               Aidan                                27 Oct 2021                                                                    G2P Part 6: Solve inconsistencies in your text with a g2p pre-processing mapping              :       This is the 6th blog post in a seven-part series about a software tool called g2p. In this post we’ll discuss how to use g2p to do the common natural. . . :                                                                               Aidan                                26 Oct 2021                                                                                                                                             G2P Part 5: Applications for g2p, an example with ReadAlongs              :       This is the 5th post in a seven-part series about g2p. In this post, we discuss using g2p to create interactive text/audio books with ReadAlong Studio. :                                                                               Aidan                                25 Oct 2021                                                                                                                                             G2P Part 4: advanced mappings with g2p              :       This is the 4th blog post in a seven-part series about g2p. This is a relatively long post, where we get in to all the nitty gritty of writing complex. . . :                                                                               Aidan                                24 Oct 2021                                                                                                                                             G2P Part 3: Make a basic mapping with g2p              :       This is the third blog post in a seven-part series describing how to make a basic mapping with g2p on your computer. If you haven’t already, please read the introductory. . . :                                                                               Aidan                                23 Oct 2021                                               &laquo; Prev       1        2        3      Next &raquo; "
    }, {
    "id": 4,
    "url": "https://blog.mothertongues.org/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "https://blog.mothertongues.org/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "https://blog.mothertongues.org/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "https://blog.mothertongues.org/g2p-case-feeding/",
    "title": "Preventing feeding of g2p rules with case",
    "body": "2021/12/07 - When you write g2p mappings, sometimes you find yourself wanting to prevent feeding between rules, but your situation is too complex to be handled by the normal prevent-feeding option. This blog post describes a technique that can be used to handle complex rule feeding scenarios. TL;DRIf you want your g2p mapping to prevent feeding between the input and output of your rules, while retaining the ability to match the output of your rules in the context of other rules, you can use this three-step mapping technique using case to prevent rule feeding, instead of a single mapping with prevent_feeding enabled. What you need to know to understand this postThis post will only make sense if you’re already an advanced user of the g2p system, and you are writing your own mappings. We assume:  Knowledge of the g2p module.  You’ve read the seven part blog post on g2p.  You know how to use context_after and context_before in g2p mappings.  You know how to write mappings in . csv files.  You know how to call g2p convert.  You’re trying to use prevent_feeding but it’s not doing what you want. Note: the word “feeding” comes from the linguistic concept of feeding order, where a rule is said to feed into another rule if it creates the context for the second rule to apply. Who is involved with this project? g2p project owner: Aidan Pine Author of this post, and g2p software developer: Eric JoanisWhat is needed to replicate the content in the post?To reproduce the examples below, you will need to install g2p on your own machine by following the instructions at GitHub/g2p and make sure the g2p convert command line works. You’ll need a recent version of g2p, since we’ll make use of the new --config option to g2p convert. Release v0. 5. 20211217 or more recent will work, or the master branch on GitHub as of 2021-12-07 or more recent. What are the motivations behind this technique?In an advanced g2p mapping scenario, some of our collaborators found themselves writing rules where, as soon as one rule matched a piece of text, no further rules should touch that text. There were many dozens of rules, each one handling some sequence of characters occurring in a specific context, with some catch-all rules at the end that applied if none of the listed contexts applied. The problem they faced was that using prevent_feeding solved making sure no part of the text was touched more than once, but it also prevented touched text from being used in the context of subsequent rules, as was necessary. Solving this complex prevent-feeding scenarioMinimal setup: For this blog post, we’ll create a fictitious scenario with a minimum number of rules exhibiting the problem found in the real use case described above. Imagine we use a g2p mapping to modify some spelling rules, where the same sequence of letters is mapped differently depending on context. In our minimal example, we want to handle “un” as follows:  when the string “atun” occurs, it should be replaced by “etun”, and the resulting “un” should not be further modified; when “un” occurs before “a” or “e”, it should remain as “un”; otherwise, “un” should be changed to “on” (catch-all rule). With these rules, the word “untunatun” should get changed to “ontunetun” because the third “un” is part of “atun”, the second “un” is followed by “a” or “e”, and only the first “un” is handled by our catch-all rule. Notice that the first two rules preserve “un” in their output, so we have to make sure the output of a rule cannot be reused as the output of a subsequent rule. The obvious solution, which does not work: We can almost solve this by using prevent_feeding, but not quite. Let’s write the rules as follows: mapping. csv: atun,etun,,un,un,,[ae]un,on,,and use config file config. yaml, where we are careful to set prevent_feeding: true: display name: mapping example for un with prevent feedingmapping: mapping. csvin_lang: l1out_lang: l2case_sensitive: falseprevent_feeding: trueWhen you run g2p convert --config config. yaml untunatun l1 l2, you will get “ontonetun” as output instead of the intended “ontunetun”. What’s the error? (I know, it’s subtle…) The middle “un” should have matched the second rule, since it was followed by “a” before the first rule was applied, and is still followed by “e” after the first rule was applied. That should have blocked the application of the third rule, but apparently it didn’t. Why is this happening? We need to talk about how prevent_feeding is implemented to answer that question. The point of the prevent-feeding option is to make sure that the output of a rule is never matched as the input of any other rule. To accomplish that, we actually map the output of each rule temporarily to characters in a Private Use Area in the Unicode standard, and map them back to the real output once all rules have been applied. The private use area characters are intended for internal (hence “private”) use within software, but should never be printed, so they were perfect to solve the prevent-feeding problem: they should never occur in input text or in the input of any rule, and so there would never unintended feeding between rules. The problem is that those private characters are also inaccessible to the context_before and context_after parts of our rules. So prevent_feeding not only blocks the characters from being matched as the input of other rules, it also blocks them from being matched in their contexts. If you want to see exactly what’s going on, run that convert command again with the --debugger option. For each rule that gets applied, you’ll see the rule, as well as the text before and after it is applied. That can help understand what’s going on whe you’re trying to debug a g2p mapping. A three-step prevent-feeding solution, which does work: The work-around we propose in this blog post is the following: Instead of defining one mapping with prevent feeding enabled, we’ll define three mappings applied one after the other, and we’ll create our own temporary representation for previously mapped characters:  mapping 1: lowercase all input text; mapping 2: make a case-sensitive mapping where the input of each rule is lowercase, the output of each rule is uppercase, and the context of each rule lists both cases, so contexts match before and after the application of other rules; mapping 3: lowercase all output text. The result exists as a test case in the g2p repo, but we’ll show it here too, a bit simplified. case-nofeed-mapping. csv: atun,ETUN,,un,UN,,[aAeE]un,ON,,case-nofeed-config. yaml: mappings: - display name: case-nofeed input lowercaser  mapping: empty. csv  in_lang: l1  out_lang: l1-lc  case_sensitive: false - display_name: main case-nofeed mapping with l1 in lc, l2 in uc, thus no feeding  mapping: case-nofeed-mapping. csv  in_lang: l1-lc  out_lang: l2-uc  case_sensitive: true - display name: case-nofeed output lowercaser  mapping: empty. csv  in_lang: l2-uc  out_lang: l2  case_sensitive: falseFor the two lowercasing mappings, notice we referred to file empty. csv. We indeed need to create an empty file called empty. csv. We’re using the fact that when a case insensitive mapping (i. e. , a mapping with case_sensitive: false) is applied, its input is lowercased before the rules are applied, so we don’t need to provide any actual rules, which means an empty CSV file will do. Now, when you run g2p convert --config case-nofeed-config. yaml untunatun l1 l2, you get the expected “ontunetun” output because for the middle “un”, the second rule matches that “E” in its context_after, since the first rule changed “atun” to “ETUN”. What if my mapping is case sensitive?: OK, so this solution depends on the fact that if your mapping is case insensitive, all input gets dropped to lower case before mapping starts, so distinctions of case in the input are not meaningful. This allows us to use upper case characters as that temporary intermediate representation that could not be matched as the input of other rules. If your mapping has to be case sensitive, then you cannot use this solution. Instead, you would have to carefully choose your own temporary representation to block feeding via your rule design, while still being able to use that temporary representation in the context of other rules. But make sure you use characters that cannot be valid input in the language you’re working with! "
    }, {
    "id": 8,
    "url": "https://blog.mothertongues.org/g2p-contributing/",
    "title": "G2P Part 7: Contributing a new mapping to g2p for everyone to use",
    "body": "2021/10/27 - This is the last part of the seven-part series on g2p. In this part, we’ll discuss how to contribute your mappings to the main g2p library. G2P Blog Series Index:  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingAdvanced: contributing your rules to the main g2p library: So, you’ve written some cool rules and you want to contribute, that’s awesome! There are lots of benefits to contributing your mapping to g2p. First of all, once your mapping is accepted, you’ll have it available and live on G2P Studio. Second, once the next version of g2p is released with your mapping, it will be automatically built in to the Convertextract library. Third, if your mapping is between a language’s writing system and the IPA, you can also get ReadAlongs support for your language. So, you write your mapping once, and you get three things for free (G2P studio, convertextract and readalongs). Here’s how:  Fork g2p, see https://docs. github. com/en/github/getting-started-with-github/fork-a-repo for more details Add a folder for your language using the appropriate ISO 639. 3 code to g2p/mappings/langs, i. e. , create the folder g2p/mapping/langs/&lt;yourlangcode&gt;/ Add a config. yaml file as described here in that folder Add your mapping in that same folder If your mapping is for an IPA mapping, you can optionally run g2p update to update your mapping into g2p and then generate the mapping as described in the ReadAlongs post between your language and English IPA.  Run g2p update to add your mapping to g2p Add some test data to g2p/tests/public/data.  Submit your changes by creating a pull requestFinally, either myself, or somebody else will review the changes, and you will get credit for those mappings and be added to the list of contributors Adding tests: Testing your work is a really important part of software engineering. It lets us make changes to code and be confident that new features don’t break the expected functionality of g2p. In order to add tests for your mapping, you can add a CSV/TSV/PSV file with 4 columns to g2p/tests/public/data. The name of the file should be just the input language code, for example fra. psv for the French tests. The first column in the file is for the input language code, the second is for the output language code, the third is for the input text and the fourth is for the expected output of that mapping and input. Here is an example between French (fra) and French IPA (fra-ipa) asserting that ‘manger’ results in ‘mɑ̃ʒe’: fra|fra-ipa|manger|mɑ̃ʒefra|fra-ipa|écoutons|ekutɔ̃There is a script for running tests at the root of the g2p project called run_tests. py. You can run all of the tests here using the following: python run_tests. py allor just run the language assertions including your tests like shown above using: python run_tests. py langsWriting g2p mappings that handle all the special cases can be quite tricky, especially when there are potential interactions between rules. To be confident that your g2p mappings work as you think, you should add a bunch of different words covering most of the spelling phenomena of the language you’re working on, with their expected IPA mapping. Ideally, you should also add some test cases to eng-ipa and eng-arpabet, to make sure the generated mapping works correctly. If you run into difficulties, feel free to post comments on this blog post or on the g2p library GitHub issues page! "
    }, {
    "id": 9,
    "url": "https://blog.mothertongues.org/g2p-preprocess/",
    "title": "G2P Part 6: Solve inconsistencies in your text with a g2p pre-processing mapping",
    "body": "2021/10/26 - This is the 6th blog post in a seven-part series about a software tool called g2p. In this post we’ll discuss how to use g2p to do the common natural language processing task of text normalization. G2P Blog Series Index:  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingAdding a ‘pre-processing’ mapping: It’s often not sufficient to just write a mapping between the characters in a language’s orthography and the IPA, as illustrated in use case #2 and use case #3 below. Real-world text input is pretty messy, and if we want ReadAlongs or Convertextract - or any other tool that uses g2p - to work properly, we need to account for as much of that messiness as possible. Generally speaking, solving this kind of messiness is usually called ‘Text Normalization’. 1 This ‘normalization’ can either be about ensuring that the same Unicode characters are used consistently, or it can also be about converting symbols into their pronounced form, like &amp; or 123. For example, maybe your language uses underlines in its orthography. There are two commonly confusable Unicode characters here: U+0331 COMBINING MACRON BELOW and U+0332 COMBINING LOW LINE, and they look almost identical (cf. g̱ (U+0331) vs g̲ (U+0332)). So, let’s ‘normalize’ to consistently use U+0331.       in   out   context_before   context_after         \u0332   \u0331           Second, maybe we have a text that has a lot of puncutation like ‘&amp;’ in it. We could write a mapping here for that as well (example in Danish):       in   out   context_before   context_after         &amp;   og           A third example can be seen in the Gitksan mapping where the writing system uses a single quote ‘ to mark ejectives and glottal stops, but there are many apostrophe-like confusable characters, like ’ or ʼ. In this mapping we can see that they’re all mapped to the single quote ‘ (U+0027). How do we link this up with the rest of our mappings? We recommend calling these mappings &lt;yourlang&gt;-equiv, for “equivalencies” which is more neutral and sometimes preferred than the term “normalization”. When you run g2p update, g2p creates a directed graph between all possible mappings. Similar to when using g2p for ReadAlongs, consider we have a g2p pipeline from ‘dan’ to ‘eng-arpabet’ that goes through the g2p graph like so, ‘dan’ → ‘dan-ipa’ → ‘eng-ipa’ → ‘eng-arpabet’. We basically want to add one more conversion along this path that does this normalization step. So, we configure a mapping for a mapping from ‘dan’ → ‘dan-equiv’ containing our normalizations, then we rename the existing mapping to ‘dan-equiv’ → ‘dan-ipa’. Then, we g2p update and the next time we run a mapping from ‘dan’ → ‘eng-arpabet’, it will pass through the normalization mapping too.       not to be confused with Unicode Normalization, which is different usage of the same term! &#8617;    "
    }, {
    "id": 10,
    "url": "https://blog.mothertongues.org/g2p-applications/",
    "title": "G2P Part 5: Applications for g2p, an example with ReadAlongs",
    "body": "2021/10/25 - This is the 5th post in a seven-part series about g2p. In this post, we discuss using g2p to create interactive text/audio books with ReadAlong Studio. G2P Blog Series Index:  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingThis post will discuss the use of g2p in the ReadAlongs project for creating interactive audio/text documents. Other Applications: There are number of different software tools that are already making use of g2p. For general purpose use in Python, have a look at this post. Convertextract: Convertextract is a tool that lets you use g2p mappings to convert text inside a variety of different documents, including Microsoft Office documents like Powerpoint and Word coduments. Have a look at the convertextract repo readme or at Fineen Davis’ blog post about the Convertextract GUI. ReadAlongs: ReadAlongs is a research project from the National Research Council’s Indigenous Language Technology Project. Communities engaged in language revitalization have potentially many different recordings of their language and some associated text, but mobilizing these materials into something that is educationally useful can be time consuming. It’s also tough from the learner’s perspective to try and follow along the text (maybe a Word Document) just by simply playing the audio. It’s easy to get lost in the recording quickly, and from personal experience, trying to do that usually involves a lot of frustrating rewinding. What if we could develop a tool that automatically figured out what parts of the audio file corresponded to what parts of the text? In Natural Language Processing, this is called forced alignment, and this is fundamentally what the ReadAlongs project does. But how does it do it? In part, by using a whole bunch of g2p mappings! First of all, it’s good to know that forced alignment is essentially a “solved problem” for languages with a lot of data, thanks to people like David Huggins-Daines who is a collaborator on the ReadAlongs project. And, if you know Python, and have sentence-aligned parallel audio/text data in your language, you can train a model using one of the several tools out there developed for this task, like the Montreal Forced Aligner. But what if you don’t have that much data (or any) and what if you don’t know Python? If your language has a g2p mapping between its writing system and the IPA, you can likely circumvent that whole process. Here’s how: We take a model for doing forced alignment on English and we manually write a g2p mapping from the writing system to IPA for the language we want to make a ReadAlong for. For example’s sake let’s say, Kanyen’kéha (Mohawk). This step requires somebody who is both familiar with the writing system used, and with the International Phonetic Alphabet. Then, we use the very cool Python library PanPhon to figure out the mapping between the IPA characters in Mohawk and their closest IPA equivalents in English. We call this mapping a “Kanyen’kéha IPA to English IPA” mapping. If you have already created your mapping between the orthography and the IPA, you can generate this “Kanyen’kéha IPA to English IPA” using the g2p command line. Then, we convert the ReadAlong in question from its original orthographic form all the way to English IPA (well, actually to ‘Arpabet’ which is an ASCII-compliant phonetic transcription standard that the alignment model understands). Then, after we “do the alignment” (ie figure out which parts of the audio correspond to which parts of the text), ReadAlongs puts it all back together again and ta-da! There is your aligned audio and text! Keep reading to see how to generate an X-to-English IPA mapping for your language. Generate your mapping between your language’s IPA and English IPA: After installing g2p and once you have made your mapping, update g2p by running g2p update. When you add or modify any mapping, g2p doesn’t actually see it yet. g2p update is what scans all the mappings and makes them available to g2p. Then, you can run g2p generate-mapping &lt;input-code&gt; --ipa. So in our example for Kanyen’kéha (moh) we would run g2p generate-mapping moh --ipaThat will automatically generate a moh-ipa to eng-ipa mapping in g2p/mappings/langs/generated - do not edit this file or its configuration unless you really know what you’re doing, because it will get overwritten. After generation, you can run g2p update again and your mapping will be usable within g2p! Visualizing the process: This is actually where the introductory picture to this blog series comes from! In the graphic below, you can see the word ‘bonjour’ in French converted to its IPA transcription. Then, its IPA transcription gets converted into its corresponding English IPA transcription based on the generated mapping (notice how uvular /ʁ/ gets transformed to alveolo-palatal /ʒ/). Then finally, the English IPA form is converted to Arpabet. So in all, we have an input of ‘bonjour’ that gets output as ‘B AO N ZH UW ZH’ and the aligner runs on that form.  You can recreate this type of animation using the G2P studio by selecting the French (fra) to English Arpabet (eng-arpabet) mapping, choosing ‘animate’ and then typing your text in the input area as seen below: What can you do with your aligned audio/text?: ReadAlongs exports to a variety of formats, including epub (for e-readers), Praat TextGrids, ELAN files, various subtitle formats, and an embeddable web component for your website. Below is an example of the embeddable web component in Danish.           Please enable JavaScript to view the ReadAlong"
    }, {
    "id": 11,
    "url": "https://blog.mothertongues.org/g2p-advanced-mappings/",
    "title": "G2P Part 4: advanced mappings with g2p",
    "body": "2021/10/24 - This is the 4th blog post in a seven-part series about g2p. This is a relatively long post, where we get in to all the nitty gritty of writing complex mappings in g2p. G2P Blog Series Index:  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingAdvanced: A deeper dive into writing tricky rules: You may have noticed that the rules described in the previous posts for converting words like ‘dog’ and ‘cat’ to IPA are woefully incomplete. The real world use cases for g2p often need to account for a lot more messiness than was described in the artificial example above. In fact, for languages like English, g2p is likely not a good solution. The English writing system is notoriously inconsistent, and there already exist a variety of other tools that account for many of the lexical (word-specific) idiosyncracies in deriving the IPA form from the orthographic form. For many Indigenous languages, the writing system is sufficiently close to the spoken form that g2p is a very appropriate solution. In the following sections, I’ll describe some common problems when writing rules, and how to fix them. As this post is quite long, please refer to the following index for quick navigation:  Rule Ordering Unicode Escape Sequences Special Settings &amp; Configuration Defining variables Regular Expresssions Using indicesRule Ordering: The order of your rules in g2p really matters! This is because some rules can either create or remove the context for other rules to apply. In linguistics, these rule ordering patterns are usually talked about as either feeding, bleeding, counter-feeding, or counter-bleeding relationships. There are potentially valid reasons to want to encode any of these types of relationships in your rules. To illustrate a possible problem, let’s consider a g2p mapping for language that converts ‘a̱’ to ‘ə’ and ‘a’ to ‘æ’. ‘a̱’ is a sequence of a regular ‘a’ followed by a combining macron below (\u0331). Because \u0331 (‘a̱’) is easily confusable with \u0332 (‘a̲’), in order to follow the rule of thumb for Unicode escape sequences, I’ll write the rules as follows:       in   out         a   æ       a\u0331   ə   Now, assuming an input to this mapping of ‘a̱’ (a\u0331), we would get ‘æ̱’ (æ\u0331) instead of ‘ə’. Why is that? Because the first rule applies and turns ‘a’ into ‘æ’ before the second rule has a chance to apply. This is called a bleeding relationship because the first rule bleeds the context of the second rule from applying. In order to avoid it, we would need to write our rules as follows:       in   out         a\u0331   ə       a   æ   With this ordering, our input of ‘a̱’ (a\u0331) would turn into ‘ə’ as we expect, and our input of ‘a’ would turn into æ also as expected. Try it out on the G2P Studio if you don’t believe me! Unicode Escape Sequences: Sometimes you need rules to convert from characters that either don’t render very well, or render in a confusing way. In those cases, you can use Unicode escape sequences. For example, maybe you want to write a rule that converts the standard ASCII ‘g’ to the strict IPA Unicode /ɡ/. As you can likely see in your browser, these characters look very similar, but they’re not the same character! The ASCII ‘g’ is U+0067 and the strict IPA ‘ɡ’ is U+0261. So, you can write a rule as follows:       in   out         \u0067   \u0261   or using JSON: [  {     in :  \u0067 ,     out :  \u0261   }]This is also helpful when you need to write rules between combining characters or other confusable characters. The rule of thumb is, if your rules are clearer using Unicode escape sequences, do it! Otherwise, just use the normal character in place. Tip for finding a character’s codepoint: If you want to find out what a particular character’s \uXXXX notation is, simply paste the character(s) into the search bar of this handy site: https://unicode. scarfboy. com/ and you will get a list of the Unicode codepoints for those characters. Note, you might find some resources that write a character’s codepoint as U+0261 instead of \u0261. The U+XXXX format is the one officially adopted by the Unicode consortium, as early as of Unicode 2. 0. 0. However, the Python programming language uses the \uXXXX format. The important part is recognizing that the Unicode codepoint is identified by the XXXX hexadecimal sequence. Special settings for your mapping configuration: You can add extra settings to your configuration file to change the way that g2p interprets your mappings. Below is a list of possible settings and their use. It’s best practise to declare all the setting keys below for each individual mapping in your config. yaml, however default values do exist. Your setting keys must be declared on the same level as all of the other keys (language_name, in_lang, out_lang etc). These settings are also available in the G2P Studio as check boxes to select or unselect. rule_ordering (default: ‘as-written’)As described in the earlier part of this post, your rules apply in the order you write them. And as described in the advanced section on rule ordering, sometimes this can make your mapping produce unexpected results! If you set your mapping to rule_ordering: 'apply-longest-first', g2p will sort all of your rules based on the length of the input to the rule, so that rules with longer inputs apply before rules with shorter inputs. This prevents some common ‘bleeding’ rule-ordering relationships described in the rule ordering section. So, if you declared your rules as: [  {     in :  a ,     out :  b   },    {     in :  ab ,     out :  c   }]Then, with rule_ordering: 'as-written' (the default), you would get ‘bb’ as the output for the input ‘ab’. Whereas with rule_ordering: 'apply-longest-first', you would get ‘c’ as the output for the input ‘ab’. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   rule_ordering: 'apply-longest-first'  # &lt;------- Add thiscase_sensitive (default: true)The default is to treat your rules as case sensitive, but setting case_sensitive: false, will make your rules case insensitive. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   case_sensitive: false  # &lt;------- Add thisescape_special (default: false)As I will describe later in the section on regular expressions, you can define rules using ‘special’ characters. By default, these characters are interpreted as ‘special’, but if you want all special characters in your mapping to be interpreted as their actual characters, you can set escape_special: true. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   escape_special: true  # &lt;------- Add thisnorm_form (default: “NFD”)If you’ve never heard of Unicode normalization don’t worry, you’re not alone! But, for writing rules and mappings using g2p, there can be some surprising ‘gotcha’ moments if you don’t choose the right normalization strategy. The basic gist of the problem is that there can be multiple ways to write the same character in Unicode, depending on whether you use ‘combining characters’ to type or not. For example, on some keyboards, you might type ‘é’ by writing an e first and then another keystroke to type the acute accent that sits above it. The Unicode representation for this would be \u0065 (e) followed by \u0301 (a combining acute accent), however there is an entirely separate Unicode code point that has these two characters pre-composed (\u00e9), which some keyboard layouts will generate instead. Many fonts will render these two different representations identically and it can be really difficult and confounding as a user if both appear in the same text. This causes problems, like text that looks identical will not appear in “find &amp; replace” or search engines will not find the text that you’re looking for, even though something that looks identical exists. Luckily, there is a standard for normalizing these differences so that all instances of sequences like \u0065\u0301 would be (NF)Composed into \u00e9, or the opposite direction where all instances of \u00e9 would be (NF)Decomposed into \u0065\u0301. For a more in-depth conversation on this, check out this blog article. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   norm_form:  NFC   # &lt;------- Add your Unicode normalization strategy hereout_delimiter (default: ‘’)Some mappings require that a delimiting character (or delimiting characters) be inserted whenever a rule applies. So, using the example from the first part of this post, maybe you want kæt to go to kʰ|æ|t instead of kʰæt. For this, you would set out_delimiter:  |  in your mapping. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   out_delimiter:  |   # &lt;------- Add your delimiter herereverse (default: false)Setting reverse: true will try to reverse the mappings so that all characters defined as out in your mapping become the input characters and vice versa. Except for a few cases, this is unlikely to work very well for advanced mapings. mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   reverse: true  # &lt;------- Add thisprevent_feeding (default: false)Let’s say you have the following rules:       in   out         kw   kʷ       k   kʲ   Let’s say the intended output here is that whenever we get a kw as an input, we get kʷ and whenever we get k we get kʲ. Ordered in the way they are defined, an input of kw will produce kʲʷ and ordered the other way, an input of kw will produce kʲw. Neither of these are correct though! So, how do we solve this? There is a setting called prevent_feeding which, if set to true, will prevent the output of one rule from being processed by any subsequent rule. As described in the rule ordering section the process when one rule provides the context for another rule to apply is called ‘feeding’. And so this setting is named prevent_feeding because it prevents that from happening. Note, setting prevent_feeding: true for your whole mapping will do this for every rule. If you just want to prevent feeding for one particular rule, you can write your rules in JSON and add the key to the specific rule you want to prevent feeding for. Prevent feeding for a single rule (in JSON rule mapping file): [  {    in :  kw ,    out :  kʷ ,    prevent_feeding : true  },  {    in :  k ,    out :  kʲ   }]Prevent feeding for every rule (in config. yaml): mappings: - language_name: English   display_name: English to IPA   in_lang: eng   out_lang: eng-ipa   type: mapping  authors:   - Aidan Pine  mapping: eng_to_ipa. json   prevent_feeding: true  # &lt;------- Add thisDefining sets of characters: Some rules are written with repeating sets of characters that can be tedious to write out. As a result, we might want to define certain sets of reusable characters using a variable name. These can be written using special types of mapping files in g2p. For example, consider a series of rules which contextually apply only between vowels. Let’s say as an example of one of those rules, that dd turns to ð when it exists between two vowels. This language has the following vowels in its inventory: a,e,i,o,u,æ,å,ø. You could write the rules like this1:       in   out   context_before   context_after         dd   ð   (a|e|i|o|u|æ|å|ø)   (a|e|i|o|u|æ|å|ø)   But, if there are lots of rules with these vowels, this could get very tedious, not to mention annoying and error-prone if the characters in the set change at some point. It is also less readable, and leaves the reader of the mapping to infer the meaning of the rule. So, in a separate file, by convention it is usually called abbreviations. csv, you can define a list of sets where each row is a new set. The first column contains the name of the set (by convention this is capitalized), and you can add characters to every following column. So, for example:       variable name                                                                     VOWEL   a   e   i   o   u   æ   å   ø                                   CONSONANT   p   b   t   d   k   g   f   s   h   v   j   r   l   m   n       FRONT   i   e   œ   ø   y                                               BACK   u   o   a                                                   Then, in your configuration, you can add the file to a specific mapping using abbreviations: abbreviations. csv. After adding it to your mapping, you can write the above rule like this instead:       in   out   context_before   context_after         dd   ð   VOWEL   VOWEL   You can also use abbreviations like this in the G2P studio by writing them in the section at the bottom of the page titled ‘Custom Abbreviations’ They will be automatically applied to your custom rules above.  Regular Expressions: Regular expressions are used ubiquitously in programming to define certain search patterns in text. In fact, this is how g2p rules work! They eventually get compiled into a regular expression too. For the most part, you can add regular expression syntax to your rules. So, suppose you wanted to write a rule that deleted word-final ‘s’, you could write the following:       in   out   context_before   context_after         s       \B   \b   As you can see in this handy regular expression cheatsheet our rule turns ‘s’ into nothing if it is preceded by \B (any character that is not a word boundary) and followed by \b (word boundary). Note: There are some ‘gotchas’ with writing regular expressions using g2p. This is a technical note, but if you’re writing some complicated regular expressions and they’re not working, don’t hesitate to raise an issue. For example there are some active issues around edge cases where regular expressions and g2p’s custom syntax for indices don’t play nice together. Using specific indices: Even people familiar with using g2p might not be aware that one of its main features is that it preserves indices between input and output segments. Meaning that when we convert from something like ‘kæt’ to ‘kʰæt’ as in the first example, g2p knows that it’s the ‘k’ that turned into the ‘k’ and ‘ʰ’ as seen below.  The default interpretation of rule indices by g2p is that it matches the characters between the input and the output one-by-one in a given rule until it reaches the end of either one, then it matches any remaining characters in the longer part (input or output) to the last character of the shorter part. For example, compare the following examples where ‘abc’ is converted to ‘ab’ and gloms the excess input character onto the last output character: and where ‘ab’ is converted to ‘abc’ and gloms the excess output character onto the last input character: But what if - for some imaginary reason - we want to show a rule where ‘ab’ turns into ‘bca’, and specifically make note that it was ‘b’ that turned into ‘bc’, and ‘a’ stays as ‘a’? Well, we can use special g2p syntax for explicitly writing these indices. Instead of,       in   out   context_before   context_after         ab   bca           we can write       in   out   context_before   context_after         a{1}b{2}   bc{2}a{1}           Now, our indices will reflect our imaginary need to index ‘a’ with ‘a’ and ‘b’ with ‘bc’: Using the explicit indices syntax will break up your rule into a number of smaller rules that apply the same defaults of above but to explicit sets of characters. You must use curly brackets, but the choice of character you put inside is arbitrary — it just has to match on both sides. By convention, we use natural numbers. This will match all the characters to the left of each pair of curly brackets in the input with the matching index in the output. So here, ‘a’ is matched with ‘a’ and ‘b’ is matched with ‘bc’. These can get fairly complicated, so we recommend only using this functionality either for demonstration purposes, or for specific applications which require the preservation of indices. Footnotes:       You’ll notice that the syntax here is a little weird, what the heck are all of those pipes (the up-down things like |) doing there? That’s because I’m using regular expressions to express a OR e OR u etc… For more info, check out the section on regular expressions &#8617;    "
    }, {
    "id": 12,
    "url": "https://blog.mothertongues.org/g2p-basic-mappings-local/",
    "title": "G2P Part 3: Make a basic mapping with g2p",
    "body": "2021/10/23 - This is the third blog post in a seven-part series describing how to make a basic mapping with g2p on your computer. If you haven’t already, please read the introductory blog post for g2p and the post about basic mappings with G2P Studio  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingWhat you need to know to understand this post   To be able to follow along, I suggest having some sort of text editor, like Visual Studio Code.     You will also need to have Python installed on your computer. If you need a bit of help, my colleague Eddie Antonio Santos wrote a very good blog article on installing Python on Mac.  What’s the gist of what we’re about to do?: As described in the previous post, in order to use g2p we need to understand the building blocks; rules and mappings. Rules are patterns that describe how to turn some input text into some other output text. When we combine a series of ordered rules together for a specific purpose, we call this a mapping. This blog post will show you how to write rules and mappings on your computer to use with g2p. How do I follow along?: You can follow along by writing your rules and mappings on your computer using a text editor like Visual Studio Code. You should also install g2p by running pip3 install g2p1 in your command line or an integrated terminal within Visual Studio Code. See this post for more information on installing Python packages with pip. Use in Python and the command line: Installation: There are two ways to install g2p:  install the latest published version of g2p, for use as is; install an editable version to create your own mappings or edit g2p yourself. To install the latest published version of g2p, we recommend installing it using pip: pip install g2pIf you are going to be creating your own mappings or editing g2p in any way, you must first fork g2p by going to https://github. com/roedoejet/g2p and forking the project to your own GitHub space. Once you’ve forked g2p, clone your own fork: git clone https://github. com/YourGitHubUsername/g2p. gitand then do an editable pip installation: cd g2p &amp;&amp; pip install -e . Usage: Using g2p within Python can be done programatically using the make_g2p function: &gt;&gt;&gt; from g2p import make_g2p&gt;&gt;&gt; transducer = make_g2p('dan', 'eng-arpabet')&gt;&gt;&gt; transducer('hej'). output_string'HH EH Y'Command Line Interface: You can also use g2p from the command line. The basic command for conversions is: g2p convert &lt;input_text&gt; &lt;in_lang&gt; &lt;out_lang&gt;So in practice: $ g2p convert hej dan eng-arpabetHH EH YYBasic Rule writing: Each rule must be defined to have a sequence of one or more input characters and a sequence of zero or more output characters. We can define these rules in g2p either using a tabular spreadsheet format (CSV) or using a format called JavaScript Object Notation or JSON. For example, rules written in the tabular comma-separated values (CSV) format:       in   out         a   æ   or using JSON: [  {     in :  a ,     out :  æ   }]Both of the above rules capture our first rule that turns an orthographic ‘a’ into a broad IPA /æ/. You can choose to write your rules in either format, although JSON will offer you slightly more flexibility when writing advanced rules. If we want to write rules that depend on a particular context, we need a couple more columns (CSV) or keys (JSON) than just in and out. This is where we use context_before and context_after. So, our second rule from above was to turn /k/ to [kʰ] when the character after /k/ is /æ/. Here, we could write the rules like this:       in   out   context_before   context_after         k   kʰ       æ   or like this using JSON: [  {     in :  k ,     out :  kʰ ,     context_after :  æ   }]Mapping configuration: When you combine multiple rules in g2p for a particular purpose, this is called a mapping. In addition to each file containing your rules, you need a configuration file that tells g2p how to process your rules. We write mapping configurations in YAML files titled config. yaml. “. yaml” is the file extension for YAML which stands for ‘Yet Another Markup Language’ - which might be how you’re feeling right now after having already learned about Comma Separated Value (CSV) files and JavaScript Object Notation (JSON)!! Here is a basic configuration for your mapping: mappings: - language_name: My Test Language # this is a shared value for all the mappings in this configuration  display_name: My Test Language to IPA # this is a 'display name'. It is a user-friendly name for your mapping.   in_lang: test # This is the code for your language input. By convention in g2p this should contain your language's ISO 639-3 code  out_lang: test-ipa # This is the code for the output of your mapping. In g2p we suffix -ipa to the in_lang for mappings between an orthography and IPA  type: mapping   authors: # This is a way to keep track of who has contributed to the mapping   - Aidan Pine  mapping: test_to_ipa. json # This is the path to your mapping file. It should be in the same folder as your config. yaml fileIf you are familiar with yaml, you will see that you can have more than one mapping under the mappings key. So to add another mapping to this file, it would look like this: mappings: - language_name: My Test Language   display_name: My Test Language to IPA   in_lang: test   out_lang: test-ipa   type: mapping   authors:   - Aidan Pine  mapping: test_to_ipa. json  - language_name: My Test Language  display_name: My Test Language IPA to Arpabet  in_lang: test-ipa   out_lang: test-arpabet  type: mapping   authors:    - Aidan Pine  mapping: test_ipa_to_arpabet. json If you’re not familiar with YAML, and you’re not just copy pasting from here, I recommend having a look at one of the many tutorials on how to use YAML properly before attempting to write your own mapping configuration, or looking at some of the examples of configurations in g2p/mappings/langs/*. Bringing it all together: From following the previous two sections, you should have two files created: test_to_ipa. json and config. yaml. Your config. yaml file should look like this: mappings: - language_name: My Test Language # this is a shared value for all the mappings in this configuration  display_name: My Test Language to IPA # this is a 'display name'. It is a user-friendly name for your mapping.   in_lang: test # This is the code for your language input. By convention in g2p this should contain your language's ISO 639-3 code  out_lang: test-ipa # This is the code for the output of your mapping. By convention in g2p we suffix -ipa to the in_lang for mappings between an orthography and IPA  type: mapping   authors: # This is a way to keep track of who has contributed to the mapping   - Aidan Pine  mapping: test_to_ipa. json # This is the path to your mapping file. It should be in the same folder as your config. yaml fileand your test_to_ipa. json file should look like this: [  {     in :  k ,     out :  kʰ ,     context_after :  æ   }]Then, type the following command in your command line or integrated terminal: g2p convert  kæt  test test-ipa --config /path/to/config. yaml, replace /path/to/config. yaml with the path from your current working directory in the command line to your config. yaml file. You should see the output kʰæt produced below. Congratulations! You did your first conversion with g2p. Try changing your rules around or converting other text and experiment to see what happens. Footnotes:       See this link for more information on the difference between pip and pip3: https://www. pythonpool. com/pip-vs-pip3/ &#8617;    "
    }, {
    "id": 13,
    "url": "https://blog.mothertongues.org/g2p-basic-mappings-gui/",
    "title": "G2P Part 2: Using G2P Studio",
    "body": "2021/10/22 - This is the second blog post in a 7-part series describing how to make a basic mapping with g2p. For background information on what g2p is, have a look at the introductory blog post for g2p  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingWhat you need to know to understand this post To be able to follow along, you’ll need access to the internet, and to have read the introductory blog post for g2pWhat’s the gist of what we’re about to do?: In order to use g2p we need to understand the building blocks; rules and mappings. Rules are patterns that describe how to turn some input text into some other output text. When we combine a series of ordered rules together for a specific purpose, we call this a mapping. This blog post will show you how to write rules and mappings to use with g2p. How do I follow along?: The easiest way to write rules quickly is using the G2P Studio web application1. Once landing on the G2P Studio page, you can scroll down to the Custom Rules section and start directly editing the spreadsheet available there. You can also follow along by writing your rules and mappings on your computer using a text editor like Visual Studio Code. This post only shows how to create rules using the G2P Studio web application. The next blog post will show you how to write these rules locally on your computer. How to get from ‘a’ to ‘b’ (or ‘a’ to ‘æ’) with g2pg2p is a Python library2 (i. e. software written in the Python programming language) that helps you convert between different characters based on user-defined rules. The inspiration for how to write these rules was mostly taken from the notion of phonological rewrite rules, which is a common way of describing multilevel phonological changes in linguistics. Multilevel changes is the idea that a word, like ‘cats’ or ‘dogs’ can have multiple ‘levels’ of representation. For example, you might think of the way that those words are written in English orthography (writing system) as one level. Then, you might think of a general pronunciation for those words, written in the phonetic alphabet, as another level. You could also separate that level into more than one level by having a level each for narrow and broad transcriptions.       Level Name   Word #1   Word #2   Word # 3         Level 1 (orthography)   cat   dog   back       Level 2 (broad IPA)   kæt   dɑɡ   bæk       Level 3 (narrow IPA)   kʰæt   dɑɡ   bæk   Now, just by looking at these three levels, you can probably see some fairly systematic rules here that you could imagine would get you from one level to the next, even if you don’t know the phonetic alphabet by heart and even if you don’t really know much about English phonology (sound patterns and systems). One possible hypothesis would be that all ‘a’ characters in level one turn to ‘æ’, so we might want a rule to express that all instances of ‘a’ turn to /æ/. And, for another example, it looks like between levels 2 and 3, /k/ turns into either [k] or [kʰ] depending on whether it occurs before or after /æ/3. So, with these hypotheses about the rules to transform from one level to another, how do we start translating this into rules for g2p? Keep reading to find out! Basic Rule writing: g2p lets you describe these patterns using an ordered series of rules. Each rule must be defined to have a sequence of one or more input characters and a sequence of zero or more output characters. In the G2P Studio, we write these rules in a spreadsheet-type interface. In the screenshot below we can see a very basic rule that will turn all instances of the character a into b. Each row is a new rule. The In column describes what characters will be matched, and the Out column describes what those matched characters will be turned into.  The Context Before and Context After columns describe any conditions for the matched text. For example, if I wanted to turn all a characters into b only if a c came before the a, I would write the rule like this: Ok, so how do I actually get these rules to do something?: So, you’ve understood the basics of writing rules described above and you want to actually use them to convert something? This section describes exactly how to do that. Below is a list of all the rules to capture the transformations between level 1 and level 2 above. There are some rules that we discussed in that section, and some others that might look unfamiliar. For a full description of some of these rules, have a look at the advanced rule-writing section.  Once you have written your rules in this section, you can write some text in the left text area at the top of the G2P Studio, and g2p will apply your rules and produce the output in the right text area as seen below: You can also click on ‘Export’ under the Custom Rules section to export your rules to a CSV file if you want to save them for later. Have a read through the next blog post on writing rules on your computer without G2P Studio. Footnotes:       I’m using the free, hobby plan at https://www. heroku. com/ to host it though, so occasionally the server goes to sleep. If you first go to the site and it takes a few seconds to boot up, don’t worry! &#8617;        A Python ‘library’ is a collection of code &#8617;        Orthographic characters are wrapped in single quotes, like ‘a’; broad IPA typically uses forward slashes like /k/ and narrow IPA typically uses square brackets like [kʰ] &#8617;    "
    }, {
    "id": 14,
    "url": "https://blog.mothertongues.org/g2p-background/",
    "title": "G2P Part 1: Getting from 'a' to 'b' with g2p - why g2p exists and will let you do awesome things",
    "body": "2021/10/21 - This is the first blog post in a seven-part series about a software tool called g2p. This post describes some of the background context for why g2p was created, and subsequent posts will go into more detail about how to use g2p. g2p is a tool for systematically converting certain characters1 into other ones. This sounds fairly simple, but it can actually be incredibly powerful and useful! For example, maybe you want to get the pronunciation from a word’s spelling, g2p can help with that! Or maybe a language you’re learning or teaching has different writing systems and you want to convert between them. Or, maybe your language has an historic or legacy way of writing and you want to convert it to the new writing system. There are also other uses for g2p which I’ll explain in following posts - keep reading to learn the basics of g2p. G2P Blog Series Index:  Background How to write a basic mapping in G2P Studio Writing mappings on your computer Advanced mappings ReadAlong Studio &amp; Other Applications Preprocessing mappings ContributingWho is involved with this project? Maintainer (i. e. the person to bug with questions): Aidan Pine Lots of other contributorsWhat are the motivations behind G2P?There are many reasons why you might want to systematically convert between different characters. Here are a few possible use cases: Use Case #1: Getting the pronunciation from a word’s spelling: Sometimes you want to convert from a language’s writing system (also known as orthography) to its pronunciation. This is a very common task in natural language processing and is essential in the creation of text-to-speech and automatic speech recognition systems. In another post in this series, I will describe the usefulness of g2p specifically with a project called “ReadAlongs”. “Letters” in a writing system are usually referred to as “graphemes” and their corresponding meaningful sounds are referred to as “phonemes”; hence “g2p” or “grapheme-to-phoneme”. It gets a little more complicated than that though, because sometimes a grapheme is made of more than one character, as with the digraph “th” which can be pronounced unvoiced as in ‘thin’ or voiced as in ‘that’. The International Phonetic Alphabet (IPA) is not so ambiguous! In IPA, the ‘th’ in ‘thin’ is written as θ and the ‘th’ in ‘that’ is written as ð. Use Case #2: A language with multiple writing systems: Some languages have two (or more!) different writing systems. Take Cree for example, where you can either write a word in Standard Roman Orthography like “ê-wêpâpîhkêwêpinamâhk” or in Syllabics like ᐁᐍᐹᐲᐦᑫᐍᐱᐊᒫᕽ. My colleague Eddie has a great blog post about a tool he created to convert between the two here. g2p can help with this kind of transformation between writing systems. Use Case #3: Converting from legacy writing systems: Some languages historically used “font hacks” to render the characters in their writing system before they were supported on computers. There’s a longer discussion to be had here, but the tldr version is that when computers were gaining widespread use among speakers of Indigenous languages, they weren’t typically able to render (i. e. , display) characters outside of the 128 characters supported by the American Standard Code for Information Interchange (ASCII) or even any of the extensions to ASCII that provide a total of 256 character (e. g. , Latin-1 for Western European languages). To get around this, language communities would come up with their own custom fonts (often referred to as “font hacks” or “font encodings”) where they would override the display of a characters like “©” which existed in Latin-1, as ‘ǧ’ instead (example taken from the Heiltsuk Doulos font). For more information on this topic, please check out ‘Seeing the Heiltsuk Orthography from Font Encoding through to Unicode’ or ‘Applications and innovations in typeface design for North American Indigenous languages’. Using g2p studioIf you want to use g2p to convert some text in one of the supported languages2, simply visit the G2P Studio, select a language from the dropdown, and type in your text, as shown below. That’s all there is to it! To learn how to add support for other languages and use g2p for other cool things, go on to the next part of the series! Footnotes:       Because the word ‘letter’ usually refers to a character within a specific alphabet or writing system, instead of ‘letter’, I’m going to use the word ‘character’ throughout this post. Similarly, despite the name of this tool being ‘Grapheme-to-Phoneme’, in reality g2p can be used to convert any characters to any other characters, not just graphemes (contrastive units of a writing system) to phonemes (contrastive units of a sound system).  &#8617;        At time of writing, this includes the following list along with their ISO-639-3 codes) alq - Anishinàbemiwin, atj - Atikamekw, crg - Michif, crj - Southern &amp; Northern East Cree, crx - Plains Cree, crm - Moose Cree, csw - Swampy Cree, ctp - Western Highland Chatino, dan - Danish, fra - French, git - Gitksan, gla - Scottish Gaelic, gwi - Gwich’in, haa - Hän, ikt - Inuinnaqtun, iku - Inuktitut, Kaska, kwk - Kwak’wala, lml - Raga, mic - Mi’kmaq, moh - Kanien’kéha, oji - Anishinaabemowin, see - Seneca, srs - Tsuut’ina, tau - Upper Tanana, tce - Southern Tutchone, ttm - Northern Tutchone, tgx - Tagish, tli - Tlingit &#8617;    "
    }, {
    "id": 15,
    "url": "https://blog.mothertongues.org/tutorial-syllabics-fonts/",
    "title": "Tutorial: Choosing the right fonts to show Cree syllabics on your website",
    "body": "2020/09/03 - tl;dr: Use thisfont-family CSS declaration to successfully display Western Cree syllabicsfor most visitors to your website: font-family: Gadugi, Euphemia, 'Euphemia UCAS', 'Aboriginal Sans', 'Noto Sans Canadian Aboriginal', sans-serif;Include the following line to your website’s &lt;head&gt; to automatically downloada syllabics font if no appropriate fonts are installed on yoursmartphone/computer/device: &lt;link rel= stylesheet  href= //fonts. googleapis. com/earlyaccess/notosanscanadianaboriginal. css &gt;Read the rest of this article to learn more about choosing syllabicsfonts for the web! What you need to know to understand this post: You will need to know how to:  How to edit your website’s CSS How to add a &lt;link&gt; element to your HTML’s &lt;head&gt;If you’ve ever added a custom font from Google Fonts to yourwebsite, you know enough to follow this post! It’s also helpful to know the difference between serif and sans-seriffonts. Let’s get started!: Have you ever wanted to display Cree syllabics on your website, butwhen you tried, all you got were these blank boxes, colloquially calledtofu?  □□□ □□□□□□! □□□□□□□□□□ □□□□□ «□ □□□□□□□□□»□ Tofu no longer! Let’s discuss the various syllabics fonts that exist outthere, where they are commonly found, and let’s create a font stack:a list of fonts that your web browser tries in order such that it findsa font that displays syllabics correctly.  Note: I am most familiar with Western Cree Syllabics (Y-dialect, Th-dialect, N-dialect).  However most of these fonts might work for other languages.  Note that some fonts in this post do not contain all syllabics characters for all languages that use syllabics, and there's even some incorrect characters in one font! 😱 So even though this is the stack I guarantee to work for Western Cree, for other languages, your mileage may vary. Common syllabics fonts: I will discuss the following fonts:  Aboriginal Sans and Aboriginal Serif Euphemia Gadugi Noto Sans Canadian AboriginalThere are many more syllabics fonts out there, but in this post, I wantto focus on just enough for you to make syllabics work on websites. I made an interactive demonstration of these fonts that you can playwith. Go ahead and type any text in Western Cree into the box and itwill be converted to syllabics and displayed in the appropriate font, ifit’s available on your system:  See the Pen  Syllabics font playground by Eddie Antonio Santos (@eddieantonio) on CodePen. But where do these fonts come from? Where should you expect these fonts to beavailable? Aboriginal Sans and Aboriginal Serif: As the names suggest, they have both serif and sans-serif variants forwriting in Latin script (e. g. , English, French, etc. ). Who created these fonts?: These fonts were created by the Language Geek himself, Chris Harvey. Where are these fonts pre-installed?: As far as I know, no system has Aboriginal Sans or Aboriginal Serifpre-installed, so you have to download them yourself from hiswebsite. Sometimes, they are bundled with certainKeyman keyboard layouts. If you’ve tried getting syllabics to work on your computer, chances are,you’ve downloaded either of these fonts! Languagegeek. com has many other fonts thatdisplay syllabics. Gadugi: This is a modern font that displays both syllabics and Cherokee. “Gadugi” is a Cherokee word that means “cooperativelabour”. Who created this font?: I believe Microsoft commissioned this font. Where is this font pre-installed?: Gadugi has been included in all versions of Windows since Windows8. It’s also available as a “Cloudfont” on Microsoft Office products. Euphemia: Euphemia or Euphemia UCAS1 is a sleek font for syllabics. It alsoincludes glyphs for Latin (e. g. , English/French) characters. Who created this font?: Tiro Typeworks created Euphemia. Ross Mills from Tiro has alsocreated the Pigiarniq for Inuktitut, but I will not cover that fontin this blog post. Where is this font pre-installed?: Euphemia is distributed on Windows Vista and newer. It is currently distributed as Euphemia UCAS on Apple systems(macOS/iOS). Issues: Unfortunately, for East Cree, this font will NOT work, as it has theincorrect orientation for the ᔓ (sho) and ᔕ (sha) syllabics 😱 So I would recommend omitting it from the font stack for EasternCree. So if you can’t use Euphemia, which font should you use instead? Noto Sans Canadian Aboriginal: Noto is Google’s project to eliminate tofu (□) for all languages of theworld, hence noto (“no more tofu”). As far as I’m aware, Google does not have a “Noto Serif CanadianAboriginal”. Who created this font?: As mentioned, Google created Noto Sans! Where is this font pre-installed?: Noto Sans Canadian Aboriginal can be found on Google products, such asAndroid phones, Android tablets, and Chromebooks. Importantly, Google allows website authors to embed Noto on yourwebsite, just as you can embed other fonts via their Google Fontsservice. Most modern web browsers will automatically download Noto Sans ifyou included it in your font stack and no appropriate font is found. How to get the right fonts to appear on your webpage: Now that we know about what fonts are out there, lets constructa font-family declaration that will support syllabics on as manydevices as possible. Recall that the font-family declaration is a list of fonts that theweb browser will try in order. The first font in the list that isinstalled will be used. Let’s start with the fallbacks first. Let’s say our users’ devicedoes not have a font that supports syllabics installed on theircomputer. Since Google allows Noto Sans Canadian Aboriginal to beautomatically downloaded on your website when needed, let’s start withthis font-family declaration: body { font-family: 'Noto Sans Canadian Aboriginal', sans-serif;}To get your browser to download the appropriate fonts, add this to yourwebsite’s &lt;head&gt;: &lt;link rel= stylesheet  href= //fonts. googleapis. com/earlyaccess/notosanscanadianaboriginal. css &gt;Add support for Aboriginal Sans: Perhaps your reader has installed Aboriginal Sans to their computer. Update your font-family declaration as follows: body { font-family: 'Aboriginal Sans', 'Noto Sans Canadian Aboriginal', sans-serif;}This way, the web browser will try using Aboriginal Sans if it’sinstalled, and then attempt to download Noto Sans. Add support for Mac/iPhone/iPad users: (omit this step for East Cree!) Now let’s support users of Apple devices. Prepend Euphemia UCAS tothe font-family declaration: body { font-family: 'Euphemia UCAS', 'Aboriginal Sans', 'Noto Sans Canadian Aboriginal', sans-serif;}Add support for Windows Users: Recall that earlier versions of Windows include Euphemia; laterversions support Gadugi. Prepend the following rules to try usingGadugi first. body { font-family: Gadugi, Euphemia, 'Euphemia UCAS', 'Aboriginal Sans', 'Noto Sans Canadian Aboriginal', sans-serif;}Finally, specify your Latin fonts: If you’re displaying Latin text (e. g. , English, French, Cree SRO, etc. ),then this font goes first. Recall that many of the syllabics fontscontain glyphs for Latin characters, so we will have to listour preferred Latin font first. For this example, say I want to use Open Sans (which you canembed from Google Fonts). Prepend Open Sans to the front ofthe list: body { font-family: 'Open Sans', Gadugi, Euphemia, 'Euphemia UCAS', 'Aboriginal Sans', 'Noto Sans Canadian Aboriginal', sans-serif;}You’re done! You’re ready to display syllabics on the web! Finished!: With these declarations, you should be able to display syllabics on yourwebsite, and be reasonably confident that everybody can read it. Here’s a complete example that uses our syllabics font stack inaddition to a few fonts from Google Fonts to round things out:  See the Pen  Syllabics font stack complete example by Eddie Antonio Santos (@eddieantonio) on CodePen.       UCAS, or “Unified Canadian Aboriginal Syllabics”, the name of the Unicode block that this font covers.  &#8617;    "
    }, {
    "id": 16,
    "url": "https://blog.mothertongues.org/convertextract-app/",
    "title": "How to use the new Convertextract application for 'quality control' of ELAN annotations",
    "body": "2020/08/13 - TL;DRHave you ever wanted to NOT spend hours tediously checking that k + ‘ is written as k̓ and not k’?If you said YES!, Convertextract is the app for you. With minimal technical knowledge, you can now systemically make your ELAN annotations consistent. What you need to know to understand this postI assume that you know some background about the g2p library. For the purposes of using these tools, a library is a collection of code and documentation, but if you would like to dig deeper you can check out this Wikipedia article The g2p library uses existing and custom mappings (i. e. arbitrary input-&gt;output conversions). For example, you might want k’ (input) to be converted to k̓ (output). The Mapping is the roadmap for converting. These conversions are arbitrary, so depending on your use case you may need to create new mappings. Most of the existing mappings convert Graphemes (a character in the writing system of a language) to Phonemes (their equivalent sound in the language), hence the name ‘g2p’. To see existing mappings click hereTo keep this post simple, I will not explain how to add new g2p mappings. The documentation for adding mappings is here Who is involved with this project?   Kwak̓wala Corpus Collection group   Sara Child saratlilinukw@gmail. com Daisy Rosenblum daisy. rosenblum@ubc. ca Caroline Running Wolf caroline. oldcoyote@gmail. com     App developer: Aidan Pine     Support for adding mappings/parsers: Fineen Davis  What is needed to replicate the content in the post? g2p Mapping of the desired conversions Language text to be converted Convertextract app (read the post for installation!)What are the motivations behind this technology?As a Student Intern on the NRC’s Indigenous Language Technology (ILT) project, I was approached by the Kwak̓wala Corpus Collection group to help create a systemic way to streamline the quality control process for their ELAN annotation data. Having many different people with many different orthographic conventions (i. e. different ways of writing the same thing) all working on annotating Kwak̓wala language data had resulted in inconsistencies. For example, there was four ways that people were writing t̓s:  t’s, t̕s, ts̓, ts’So, I added mappings in the g2p library that took the alternative forms and streamlined them therefore producing only one form in the output. Then I added support for ELAN files in the Convertextract library, so that the process became automated. Aidan Pine then turned Convertextract into an app! How to use the new Convertextract app for ‘quality control’ of ELAN annotationsConvertextract, created by Aidan Pine, is a python library which extracts text data and finds/replaces specific text based on arbitrary correspondences. Until now, only basic CLI (Command Line Interface) was supported. Using Convertextract in the CLI allowed the user to convert a file based on pre-existing Mappings in the g2p library or based on a custom Mapping (not described here). However, the downside is that some programming knowledge is needed to use the CLI. The latest update now includes a GUI (Graphical User Interface) in the form of an app (for Mac computers only). The app makes Convertextract more accessible for non-programmers. 1. G2P mapping: Convertextract will carry out the streamlining for you, but it has to know what to convert. The g2p Mapping is this roadmap. See the section What you need to know to understand this post for more information on how to see if your language is supported. 2. Language data: You language data must be in one of the supported file formats. The most recent addition is . eaf files, which allows ELAN annotations to be used!For a full list of supported file types click here. 3. Convertextract application: Installation: IMPORTANT The app works on Mac only!!! To download the app: https://github. com/roedoejet/convertextract/releases In your downloads folder, find the . zip file and double click on it to unzip.  Downloads&gt;convertextractRight-click on the application in the dist folder and select Open.  Downloads&gt;convertextract&gt;dist&gt;ConvertextractNote: If you try to double click to open the app, you will get a security message. Right-clicking to open will allow you to override the security message. Using the app: This is what the app looks like when you open it.  All you have to do is add your language data, choose the encoding (usually ‘utf-8’ should suffice), and pick your g2p mapping!The output will be exported as a copy of the input file + _converted. ext in the filename. Example caseWhen typing, there is more than one way to write k̓ in the Kwak̓wala language. Convertextract takes all of these possibilities and generates one output for the sake of consistency. I used the following inputs for Convertextract:  Encoding: utf-8 Input_language: kwk-umista  Output_language: kwk-umista-conPerforming ‘quality control’           Input language   Output language         Language code   kwk-umista   kwk-umista-con       Sample text   kwak’wala   kwak̓wala           kwak]wala   kwak̓wala   If you need help setting up the app or have any questions at all, please feel free to comment below or send me an email! "
    }, {
    "id": 17,
    "url": "https://blog.mothertongues.org/why-a-new-cree-syllabics-converter/",
    "title": "Why I made yet another Cree syllabics converter",
    "body": "2020/08/07 - The Western Cree languages—Plains Cree, Woods Cree, and Swampy Cree—are written using two systems: one with letters borrowed from theEnglish alphabet, in a system known as the standard Roman orthography (SRO), and ᓀᐦᐃᔭᐏ ᒐᐦᑭᐯᐦᐃᑲᓇ (Cree syllabics). SRO is relatively easy to type on a modern computer, but syllabics are more difficult, because of the lack of a well-established syllabics input layout. It’s easier to just use a converter which, given Cree text in SRO, produces Cree text in syllabics. In this post, I describe my criticisms of the converters that existed prior to July 2018, and introduce syllabics. app—a syllabics converter that I developed in reaction to the former converters. What you need to know to understand this post: For the latter part of this blog post, where I discuss using theconverter in your own project, I assume you know how to use:  npm for installing JavaScript packages; or pip for installing Python packagesIf you don’t care about embedding my converter in your coding project,then there’s no prior technical knowledge needed! What are the freely available transliterators?: A quick Google search will net you at least the following SRO to syllabics transliterators.  The Maskwacîs Plains Cree Syllabic Converter1 The Algonquian Linguistic Atlas Cree Syllabics Converter Syllabics. net’s Plains Cree Syllabics ConverterHowever, none of these transliterators are perfect. The issues: Word final “hk”: In syllabics, a word that ends with an “hk”—or «ᐠ» in syllabics—aresupposed end with «ᕽ» instead. However, this replacement can never occur in the middle of a word. For example, the word “ê-wêpâpîhkêwêpinamâhk” (we (and not you) are setting it swinging), contains both a final “hk” and a “hk” cluster in the middle of the word. Its syllabic transcription is ᐁᐍᐹᐲᐦᑫᐍᐱᐊᒫᕽ. The Algonquian Linguistic Atlas’s converter and syllabics. net’s converter both handle the conversion of “hk” to «ᐠ», without erroneously converting the sequence in the middle of a word. Notably, the community of Maskwacîs does not follow this convention. Therefore, the Maskwacîs Converter produces ᐁ ᐁᐧᐸᐱᐦᑫᐁᐧᐱᓇᒪᐦᐠ, unlike what is expected in other Cree communities. Transliterating non-Cree words: Some transliterators attempt to convert every Latin character, even if it doesn’t make sense. Take the case of “Maskêkosihk Trail”—a road that goes from Edmonton to Enoch Cree Nation. The City of Edmonton unveiled the street sign, and, in the process, they unveiled an embarrassment:     “Maskêkosihk trail” erroneously converted as «ᒪᐢᑫᑯᓯᐦᐠ ᐟrᐊᐃl»   Image source: CBC   Not only does the syllabics transliteration of the sign contain the “hk”cluster as mentioned above,2 but it half-transliterates the English word “trail” into syllabics. The result is that “trail” is rendered as «ᐟrᐊᐃl», which contains Latin characters in the transliteration! In my opinion, an SRO to syllabics transliterator should refuse to transliterate words that do not have the structure of a Cree word. However, all three of the mentioned transliterators do attempt to transliterate “trail” with differing results:3       Maskwacîs Cree Dictionary   ᐟrᐊᐃl       Algonquian Linguistic Atlas   ᐟᕒᐊᐃᐪ       Syllabics. net   ᐟᕒᐊᐃᓬ   Long vowels: Long vowels (êîôâ) are distinct from short vowels (ioa) in Cree. Long vowels are written with a dot above in syllabics. The exception is for “ê” because it is always long; as a result, some writers also drop the diacritic when writing “e” in SRO as well. It’s important to differentiate between long and short vowels, because it makes distinctions between words. For example, nipiy/ᓂᐱᕀ means “water” while nîpiy/ᓃᐱᕀ means “leaf”. However, there is such a thing as “plain” script, where the vowel dots are omitted, and pointed script where the vowels have all dots. Another complication is that the “standard” Roman orthography in practice has multiple conventions for writing long vowels: using a macron (◌̄) and using a circumflex (◌̂). 4 How do the various converters handle long vowel diacritics? The Maskwacîs converter does not produce dots for long vowels at all, however it accepts both macrons and circumflexes as input. The Algonquian Lingustic Atlas’s converter not only produces dots, but supports input in either macrons or circumflexes. The syllabics. net converter does worst of all, handling only macrons for long vowels. It simply spits out characters written with circumflexes. Additionally, it does not handle “ê” without an diacritics, which all other converters do. Other odds and ends: Other issues for syllabics converters include how they deal with dashes, how they deal with combining diacritics, rather than pre-composed characters, and whether they produce the correct Unicode characters for the syllabics rather than very convincing look-alikes. There’s also the sandhi orthographic rule, but honestly, I’m not sure I fully comprehend how to apply this rule myself. Summary: Here’s a breakdown of the previous issues, and whether each transliterator can handle it correctly.           Word-final “hk”   Non-Cree words   Long vowels         Maskwacîs Cree Dictionary   *   ❌   ❌       Algonquian Linguistic Atlas   ✅   ❌   ✅       Syllabics. net   ✅   ❌   ❌   Where’s the source code?: The most pressing issue to me personally is that I cannot find source code for any of these converters! This means that if other people want to incorporate a converter into their own app without an active internet connection, they can’t. They have to either reverse-engineer the converters online, or write their own code to do the conversion. cree-sro-syllabics: an open-source Python and JavaScript library for syllabics conversion: My solution was to create a code library that is free and open source. It is available both for Python and JavaScript, and you can try it out right now! It handles all the issues previously mentioned. Try it with the following test cases:  Maskekosihk trail êwêpâpîhkêwêpinamahk ēwēpâpīhkēwēpinamahk ewepapihkewepinamahkThe source code for cree-sro-syllabics can be found on their respective GitHub repositories:  cree-sro-syllabics for Python cree-sro-syllabics for JavaScriptBut it can also be seamlessly incorporated into a Python project that uses pip by installing it with: pip install cree-sro-syllabicsOr, you can use npm to install cree-sro-syllabics in your JavaScript project: npm install cree-sro-syllabics --saveOr you can copy-paste the . js file to your project. Use cases: Most folks will just use syllabics. app to convert a few words orsentences of Cree. However, software developers can embed the converter in clever ways in their application. For example, I’ve used the converter in itwêwina — the Plains Cree dictionary. Although the underlying dictionary content is written entirely in SRO, we can present all Cree text in syllabics. In addition, we support searches in syllabics by using cree-sro-syllabics to convert the search string to SRO first, then search our dictionary content. How can you use cree-sro-syllabics?  Note: This blog post has been adapted from a post on Eddie’s blog.       This is the same converter bundled in the Cree Dictionary app.  &#8617;        This may not be a mistake; they could be using Maskwacîs’s conventions, but I’m really not sure.  &#8617;        I strongly suspect the sign designer used the Maskwacîs transliterator to get this result.  &#8617;        Anecdotally, I find that most writers near Edmonton and Maskwacîs prefer circumflexes to macrons; however noted Algonquian linguist Arok Wolvengrey prefers macrons. Heck, Jean Okimāsis writes her surname with a macron! &#8617;    "
    }, {
    "id": 18,
    "url": "https://blog.mothertongues.org/post-template/",
    "title": "Standard post template",
    "body": "2020/07/31 - This blog post describes what a basic template for a post on this blog should look like. Feel free to just copy past the headers into your post and replace the text! The instructions for each section are italicized, and the answers for this specific blog post are following the italicized text. TL;DRWe recommend having a section at the top that gives an extremely short summary of the post (ideally 2 or 3 sentences). This should be high-level, and shouldn’t assume any technical knowledge. This is a template of a blog post to follow when writing other blog posts - how meta! What you need to know to understand this postYour post should have a list of technical skills that you think are needed to understand the blog post. This will help the reader know if they’ll be able to benefit from reading it or whether they should study up on something first.  Knowledge of writing markdownWho is involved with this project?Beyond the author’s information which will be part of this post, there should be a list of everybody involved with the technology discussed in the post, if applicable. The list of contacts should be clear  Author/Blog Maintainer: Aidan PineWhat is needed to replicate the content in the post?Is the technology or tip you’re describing reproducible? If so, what are the requirements? For example, is it available for any language, given 20 hours of audio data? You will need to follow the steps of becoming an author and writing a post before using this template. What are the motivations behind this project/technology/tip?Was this project funded? By whom? What were the explicit goals of the technology in question, or are they left unstated? Mother Tongues was created to release open-source software for language revitalization and includes tools for dictionaries and [orthography converters][https://github. com/roedoejet/convertextract]. Please read the About section for more information. Main Post &lt;– replace titleHere is where the main post should go - because this is just a template, there’s nothing here! How to add an image to your post:    Place your image in assets/images/. I recommend prefixing your image name with your post title, followed by two dashes. For example, the post title for this very post you are reading right now is post-template. I have an image called ime-development. jpg, so I’ll copy it as assets/images/post-template--ime-development. jpg.     Embed your picture using the {% picture %} tag. In your post,where you want your image, embed it as follows:  {% picture post-template--ime-development. jpg --alt An early draft storyboard of how to use the IME %}It will produce the following image: The text after the --alt is the text that will show up if the imagedoesn’t load, or the text that screenreader users will hear whenlistening to the blog post. "
    }, {
    "id": 19,
    "url": "https://blog.mothertongues.org/write-a-post/",
    "title": "Write a post",
    "body": "2020/03/17 - Here’s a short tutorial on how to write your first blog post on the official Mother Tongues Blog. This tutorial assumes you’ve already signed up to become an author. Posts on the Mother Tongues Blog are written in Markdown. In order to write a new post, follow the following steps.  In your fork of the Mother Tongues Blog Respository, make sure you’re in the dev. author branch and add a new post to the _posts folder. Your post file name must by slugified. It must start with the date (yyyy-mm-dd) and then the blog post name, 2020-01-15-this-is-a-sample. md.  Add some meta data about the post at the top:  ---layout: posttitle:  Become an author author: aidancategories: [ Tutorial, Blog ]tags: [ intermediate ]image: assets/images/01. jpgdescription:  Write your own articles for the Mother Tongues Blog featured: falsehidden: false---    Write the content of your post in Markdown. Please have a look at the template for writing accessible posts! When you’re happy with it, Submit a pull requestThanks for contributing! "
    }, {
    "id": 20,
    "url": "https://blog.mothertongues.org/welcome/",
    "title": "Welcome",
    "body": "2020/03/17 - Welcome to the Mother Tongues official blog! Mother Tongues is an organization that releases free and open source tools for language revitalization. Here you’ll find blog posts, tips, tricks and tutorials for developing language technology with a focus on Indigenous languages. Be sure to sign up to our mailing list to keep track of new blog posts and any events. If you’re interested in becoming an author, and you have a GitHub account, please checkout how to become an author and after you’ve signed up, learn how to write a post. "
    }, {
    "id": 21,
    "url": "https://blog.mothertongues.org/become-an-author/",
    "title": "Become an author",
    "body": "2020/03/17 - Do you have a tip you’d like to share? Have you pulled your hair out fixing a bug only to find out that the reason the bug exists is because many mainstream platforms don’t consider less-resourced languages? The Mother Tongues blog is the place to share your tips, tricks, and tutorials for all things related to technology for less-resourced languages. To become an author, you’ll need a GitHub account1. Then, follow these steps:  Fork the Mother Tongues Blog Respository Create a branch called dev. author and check it out   Edit the file labelled _config. yml. * denotes a required value.   authors: AidanPine:  name: Aidan  display_name: Aidan  gravatar: 7623fd3eeb0acbe1084fecc20c3093ae   email: hello@aidanpine. ca  web: https://aidanpine. ca  twitter: https://twitter. com/aidanpine  description:  Lead developer of Mother Tongues.   yourname*:       # This should be your twitter handle   name: YourName*   display_name: YourName*   email: your@email. com*   gravatar: YourGravatarID # this is an md5 hash of your email that you used to sign up for https://en. gravatar. com/ you can either calculate this on the command line, or use an online generator like https://www. md5hashgenerator. com/   web: yoursite. com   twitter: https://twitter. com/yourhandle   description:  Guest Author. YourDescriptionHere       Add your name to the list of authors in admin/config. yml   - { label: 'Author', name: 'author', widget: 'select', options: [ 'AidanPine',  '_eddieantonio', 'delaney', 'fineen', 'YOURNAME' ] }    Submit a pull request to merge your changes in dev. author into the review branch. That’s it! Start [writing your posts](https://blog. mothertongues. org/write-a-post).   Footnotes: 1: Any suggestions for good GitHub tutorials? Leave them in the comments below! "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});