---
layout: post
title:  "Getting from 'a' to 'b' with g2p"
author: AidanPine
categories: []
tags: [ tutorial, explanation, tech-showcase ]
image: assets/images/bonjour-g2p.png
description: "How to get from 'a' to 'b' using the g2p library"
featured: false
hidden: true
---

This blog post describes the background context for a software tool called `g2p` which is shorthand for 'Grapheme-to-Phoneme', but if you don't know those terms, don't worry, this post will explain it. Essentially, `g2p` is a tool for systematically converting between two types of text. Common use cases include converting between a [writing system and the phonetic alphabet](#use-case-1-getting-the-pronunciation-from-a-words-spelling), between two [writing systems for the same language](#use-case-2-a-language-with-multiple-writing-systems), or between a legacy ['font-hacked' writing system and its current Unicode-supported version](#use-case-3-converting-from-legacy-writing-systems).

# What you need to know to understand this post

- To understand the advanced section, you will need to know some [Python](https://www.python.org/)

# Who is involved with this project?

- Maintainer (i.e. the person to bug with questions): [Aidan Pine](https://aidanpine.ca)
- Lots of [other contributors](https://github.com/roedoejet/g2p/graphs/contributors)

# What are the motivations behind G2P?

There are many reasons why you might want to systematically convert between different characters[^1]. Here are a few possible use cases:

### Use Case #1: Getting the pronunciation from a word's spelling

Sometimes you want to convert between a language's writing system (also known as *orthography*) and its pronunciation. "Letters" in a writing system are usually referred to as "graphemes" and their corresponding meaningful sounds are referred to as "phonemes"; hence "g2p" or "grapheme-to-phoneme". 

It gets a little more complicated than that though, because sometimes a grapheme is made of more than one character, as in "th" which can be pronounced *[unvoiced](https://www.thoughtco.com/voiced-and-voiceless-consonants-1212092#:~:text=Voiceless%20consonants%20do%20not%20use,as%20in%20%22thing%22).)* as in 'thin' or *[voiced](https://www.thoughtco.com/voiced-and-voiceless-consonants-1212092#:~:text=Voiced%20Consonants,-Your%20vocal%20cords&text=As%20you%20pronounce%20a%20letter,W%2C%20Y%2C%20and%20Z.)* as in 'that'. 

The [International Phonetic Alphabet](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet) is not so ambiguous, and writes the 'th' in 'thin' as **θ** and the 'th' in 'that' as **ð**. Knowing how to convert between the written and spoken form of a language is useful in a variety of computational tasks, but I will describe the usefulness specifically with a project called "ReadAlongs" [below](#readalongs).

### Use Case #2: A language with multiple writing systems

Some languages have two (or more!) different writing systems. Take Cree for example, where you can either write a word in *Standard Roman Orthography* like "ê-wêpâpîhkêwêpinamâhk" or in *Syllabics* like **ᐁᐍᐹᐲᐦᑫᐍᐱᐊᒫᕽ**. My colleague [Eddie](https://eddieantonio.ca) has a great blog post about a tool he created to convert between the two [here]({{ "why-a-new-cree-syllabics-converter" | absolute_url }}).

### Use Case #3: Converting from legacy writing systems

Some languages historically used 'font hacks' to render the characters in their writing system before they were supported standardly on computers. There's a longer discussion to be had here, but the coles' notes version is that when computers were gaining popularity, they weren't typically able to *render* (ie display) characters outside of the 128 characters supported by the [American Standard Code for Information Interchange (ASCII)](https://en.wikipedia.org/wiki/ASCII). To get around this, language communities would come up with their own custom fonts (often referred to as 'font hacks' or 'font encodings') where they would render characters like '©' which existed in ASCII as 'ǧ' instead (example taken from the Heiltsuk Doulos font). For more information on this topic, please check out ['Seeing the Heiltsuk Orthography from Font Encoding through to Unicode'](http://aidanpine.ca/static/cv/pdfs/Pine-Turin-Convertextract-2018.pdf) or ['Applications and innovations in typeface design for North American Indigenous languages'](https://markturin.sites.olt.ubc.ca/files/2020/06/Schillo_Turin_typeface_2020.pdf).

# How to get from 'a' to 'b' (or 'a' to 'æ') with g2p

`g2p` is a Python library[^2] that helps you convert between different characters based on user-defined rules. The inspiration for how to write these rules was mostly taken from the notion of [phonological rewrite rules](https://en.wikipedia.org/wiki/Phonological_rule), which is a common way of describing 'multi-level' phonological changes in linguistics. What is meant by that is the idea that a word, like 'cats' or 'dogs' can have multiple 'levels' of representation. For example, you might think of the way that those words are written in English orthography (writing system) as one level. Then, you might think of a general pronunciation for those words, written in the phonetic alphabet, as another level. You could also separate that level into more than one level by having a level each for [narrow and broad transcriptions](https://en.wikipedia.org/wiki/Phonetic_transcription#Narrow_versus_broad_transcription:~:text=orthography.-,Narrow%20versus%20broad%20transcription).

| Level Name |  Word #1   |  Word #2  | Word # 3 |
|---|---|---|---|
| Level 1 (orthography) | cat | dog | back |
| Level 2 (broad IPA) | kæt | dɑɡ | bæk |
| Level 3 (narrow IPA) | kʰæt | dɑɡ | bæk |

Now, just by looking at these three levels, you can probably see some fairly systematic rules here that you could imagine would get you from one level to the next, even if you don't know the phonetic alphabet by-heart and even if you don't really know about the English phonology (sound patterns and systems).

First of all, it looks like all of the 'a' characters in level one turn to 'æ', so we might want a rule to express that all instances of 'a' turn to /æ/. And, for another example, it looks like between levels 2 and 3, /k/ turns into either [k] or [kʰ] depending on whether it occurs before or after /æ/[^3]. So, with these hypotheses about the rules to transform from one level to another, how do we start translating this into rules for `g2p`?

## Basic Rule writing

`g2p` lets you describe these patterns using an ordered list of rules. Each rule must be defined to have an sequence of one or more input characters and a sequence of zero or more output characters. We can define these rules in `g2p` either using a tabular, spreadsheet format (csv) or using a format called JavaScript Object Notation or [JSON](https://www.json.org/json-en.html). To actually write these rules along with this blog post, I recommend getting some sort of text editor, like [Visual Studio Code](https://code.visualstudio.com/). You could also write the spreadsheet-type rules in your favourite spreadsheet editor.

| in | out |
|---|---|
| a | æ |

or using [JSON](https://www.json.org/json-en.html):

```json
[
    {
        "in": "a",
        "out": "æ"
    }
]
```

Both of the above rules capture our first, rule that turns an orthographic 'a' into a broad IPA /æ/. You can choose to write your rules in **either** format, although JSON will offer you slightly more flexibility when writing [advanced rules](#advanced-a-deeper-dive-into-writing-tricky-rules).

If we want to write rules that depend on a particular context, we need a couple more columns (csv) or keys (JSON) than just `in` and `out`. This is where we use `context_before` and `context_after`. So, our second rule from above was to turn /k/ to [kʰ] when the character after /k/ is /æ/. Here, we could write the rules like this:

| in | out | context_before | context_after |
|---|---|---|---|
| k | kʰ | | æ |

or like this using JSON:

```json
[
    {
        "in": "k",
        "out": "kʰ",
        "context_after": "æ"
    }
]
```

## Ok, so how do I actually get started here?

So, you've understood the [basics of writing rules](#basic-rule-writing) either described above and you want to actually use them to convert something! This section describes exactly how to do that. 

The easiest way to write rules quickly is using the [G2P Studio](https://g2p-studio.herokuapp.com/) web application[^4]. Once landing on the G2P Studio page, you can scroll down to the [Custom Rules](https://g2p-studio.herokuapp.com/#out_delimiter-0:~:text=Custom%20Rules) section and start directly editing the spreadsheet available there. Below is a list of all the rules to capture the transformations between level 1 and level 2 above. There are some rules that we discussed in that section, and some others that might look unfamiliar. For a full description of some of these rules, have a look at the [advanced rule-writing section](#a-deeper-dive-into-writing-tricky-rules).

{% picture custom-rules-g2p-studio.png %}

Then, you can write some text in the left text-area at the top of the G2P Studio, and g2p will apply your rules and produce the output in the right text area as seen below:

{% picture custom-rules-output-g2p-studio.png %}

You can then click on 'Export' under the Custom Rules section to export your rules to a CSV file if you want to save them for later.

## Mapping configuration

When you combine multiple rules in `g2p` for a particular purpose, this is called a *mapping*. In addition to each file containing your rules, you need a configuration file that tells `g2p` how to process your rules. We write mapping configurations in files titled `config.yaml`. You will need some sort of text editor, like [VS Code](https://code.visualstudio.com/) in order to edit your configuration file. 

Here is a basic configuration for your mapping:

```yaml
mappings:
  - language_name: English # this is a shared value for all the mappings in this configuration
    display_name: English to IPA # this is a 'display name'. It is a user-friendly name for your mapping.
    in_lang: eng # This is the code for your language input. By convention in g2p this should contain your language's ISO 639-3 code
    out_lang: eng-ipa # This is the code for the output of your mapping. By convention in g2p we suffix -ipa to the in_lang for mappings between an orthography and IPA
    type: mapping 
    authors: # This is a way to keep track of who has contributed to the mapping
      - Aidan Pine
    mapping: eng_to_ipa.json # This is the path to your mapping file. It should be in the same folder as your config.yaml file
```

If you are familiar with yaml, you will see that you can have more than one mapping under the `mappings` key. So to add another mapping to this file, it would look like this:

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping 
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
  - language_name: English
    display_name: English IPA to Arpabet
    in_lang: eng-ipa 
    out_lang: eng-arpabet
    type: mapping 
    authors: 
      - Aidan Pine
    mapping: eng_ipa_to_arpabet.json 
```

### Special settings

You can add extra settings to your configuration file to change the way that `g2p` interprets your mappings. Below is a list of possible settings and their use. All of the settings keys below must be declared for each individual mapping in your `config.yaml` and must be declared on the same level as all of the other keys (`language_name`, `in_lang`, `out_lang` etc). These settings can also be set in the `G2P Studio` instead of in a `config.yml` file.

##### `as_is` (default: True)
As described in the earlier part of this post, your rules apply in the order you write them. And as described in the advanced section on [rule ordering](#rule-ordering), sometimes this can make your mapping produce unexpected results! 

If you set your mapping to `as_is: false`, `g2p` will sort all of your rules based on the length of the input to the rule, so that rules with longer inputs apply before rules with shorter inputs. This prevents some common 'bleeding' rule-ordering relationships described in the [rule ordering](#rule-ordering) section. So, if you declared your rules as:

```json
[
    {
        "in": "a",
        "out": "b"
    },
        {
        "in": "ab",
        "out": "c"
    }
]
```

Then, with `as_is` set to True (the default), you would get 'bb' as the output for the input 'ab'. Whereas with `as_is` set to False, you would get 'c' as the output for the input 'ab'.

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    as_is: false   # <------- Add this
```

##### `case_sensitive` (default: True)

The default is to treat your rules as case sensitive, but setting this to False, will make your rules case insensitive.

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    case_sensitive: false   # <------- Add this
```

##### `escape_special` (default: False)

As described in the section on [regular expressions](#regular-expressions), you can define rules using 'special' characters. By default, these characters are interpreted as 'special', but if you want all special characters in your mapping to be interpreted as their actual characters, you can set `escape_special` to true.

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    escape_special: true   # <------- Add this
```

##### `norm_form` (default: "NFD")

If you've never heard of Unicode normalization don't worry, you're not alone! But, for writing rules and mappings using `g2p`, there can be some surprising 'gotcha' moments if you don't choose the right normalization strategy. 

The basic gist of the problem is that there can be multiple ways to write the *same* character in Unicode, depending on whether you use 'combining characters' to type or not. For example, on some keyboards, you might type 'é' by writing an e first and then another keystroke to type the acute accent that sits above it. The Unicode representation for this would likely be \u0065 (e) followed by \u0301 (a combining acute accent), however there is an entirely separate Unicode code point that has these two characters *pre-composed* (\u00e9). 

Many fonts will render these two different representations identically and it can be really difficult and confounding as a user if both appear in the same text. Often times find/replace won't work and frustrating things like that. Luckily, there is a standard for normalizing these differences so that all instances of sequences like \u0065\u0301 would be (NF)Composed into \u00e9, or the opposite direction where all instances of \u00e9 would be (NF)Decomposed into \u0065\u0301. For a more in-depth conversation on this, check out [this blog article](https://withblue.ink/2019/03/11/why-you-need-to-normalize-unicode-strings.html).

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    norm_form: "NFC"   # <------- Add your Unicode normalization strategy here
```

##### `out_delimiter` (default: '')

Some mappings require that a delimiting character (or characters) be inserted in between when a rule applies. So, using the first example, maybe you want `kæt` to go to `kʰ|æ|t` instead of `kʰæt`. For this, you would set `out_delimiter: "|"` in your mapping.

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    out_delimiter: "|"   # <------- Add your delimiter here
```

##### `reverse` (default: False)

Setting this to reverse will try to reverse the mappings so that all characters defined as `out` in your mapping become the input characters and vice versa. Except for a few cases, this is unlikely to work very well for advanced mapings.

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    reverse: true   # <------- Add this
```

##### `prevent_feeding` (default: False)

Let's say you have the following rules:

| in | out |
|---|---|
| kw | kʷ |
| k | kʲ |

Ordered in the way they are defined, an input of `kw` will produce `kʲʷ` and ordered the other way, an input of `kw` will produce `kʲw`. Neither of these are correct though! For this mapping, we want `kʷ` as the output. So, how do we solve this? There is a setting called `prevent_feeding` which, if set to true, will prevent the output of one rule from being processed by any subsequent rule. As described in [rule order](#rule-ordering) this process when one rule provides the context for another rule to apply is called 'feeding' and so this setting will prevent that from happening. Note, setting `prevent_feeding: true` for your whole mapping will do this for every rule. If you just want to prevent feeding for one particular rule, you can write your rules in JSON and add the key to the specific rule you want to prevent feeding for.

```json
[
   {
      "in": "kw",
      "out": "kʷ",
      "prevent_feeding": true
   },
   {
      "in": "k",
      "out": "kʲ"
   }
]
```

```yaml
mappings:
  - language_name: English 
    display_name: English to IPA 
    in_lang: eng 
    out_lang: eng-ipa 
    type: mapping
    authors:
      - Aidan Pine
    mapping: eng_to_ipa.json 
    prevent_feeding: true   # <------- Add this
```

## Advanced: A deeper dive into writing tricky rules

You may have noticed that the rules described above for converting words like 'dog' and 'cat' to IPA are woefully incomplete. The real world use cases for `g2p` often need to account for a lot more messiness than was described in the artificial example above. In fact, for languages like English, `g2p` is likely **not** a good solution. The English writing system is notoriously inconsistent, and there already exist a variety of other tools that account for all of the lexical (word-specific) idiosyncracies in deriving the IPA-form from the orthographic form. For many Indigenous languages, the writing system is sufficiently close to the spoken form that `g2p` is a very appropriate solution. In the following sections, I'll describe some common problems when writing rules, and how to fix them.

### Unicode Escape Sequences

Sometimes you need rules to convert from characters that either don't render very well, or render in a confusing way. In those cases, you can alternatively use [Unicode escape sequences](https://www.rapidtables.com/code/text/unicode-characters.html). For example, maybe you want to write a rule that converts the standard ASCII 'g' to the strict IPA Unicode /ɡ/. As you can likely see on your browser, these characters look very similar, but they're not! The ASCII 'g' is U+0067 and the strict IPA 'ɡ' is U+0261. So, you can write a rule as follows:


| in | out |
|---|---|
| \u0067 | \u0261 |

or using [JSON](https://www.json.org/json-en.html):

```json
[
    {
        "in": "\u0067",
        "out": "\u0261"
    }
]
```

This is also helpful when you need to write rules between combining characters or other confusable characters. The **rule of thumb** is, if your rules are clearer using Unicode escape sequences, do it! Otherwise, just use the normal character in place.

### Rule Ordering

The order of your rules in `g2p` really matters! This is because some rules can either create or remove the context for other rules to apply. In linguistics, these rule ordering patterns are usually talked about as either [feeding, bleeding, counter-feeding, or counter-bleeding](https://linguistics.stackexchange.com/questions/6084/whats-the-difference-between-counterbleeding-bleeding-and-feeding) relationships. There are potentially valid reasons to want to encode any of these types of relationships in your rules. 

To illustrate a possible problem, let's consider a `g2p` mapping for language that converts 'a̱' to 'ə' and 'a' to 'æ'. 'a̱' is a sequence of a normal a followed by a combining macron below (\u0331). Because \u0331 ('a̱') is easily confusable with \u0332 ('a̲'), in order to follow the rule of thumb for [Unicode escape sequences](#unicode-escape-sequences), I'll write the rules as follows:

| in | out |
|---|---|
| a | æ |
| a\u0331 | ə |

Now, assuming an input to this mapping of 'a̱' (a\u0331), we would get 'æ̱' (æ\u0331) instead of 'ə'. Why is that? Because the first rule applies and turns 'a' into 'æ' before the second rule has a chance to apply. This is called a 'bleeding' relationship. In order to avoid it, we would need to write our rules as follows:

| in | out |
|---|---|
| a\u0331 | ə |
| a | æ |

With this ordering, our input of 'a̱' (a\u0331) would turn into 'ə' as we expect, and our input of 'a' would turn into æ also as expected. Try it out on the [G2P Studio](https://g2p-studio.herokuapp.com) if you don't believe me!

### Defining sets of characters

Some rules are written with repeating sets of characters that can be tedious to write out. For this, we might want to define certain sets of characters using a *variable* name. These can be written using special types of mapping files in `g2p`.

For example, consider a series of rules which contextually apply only between vowels. Let's say as an example of one of those rules, that `dd` turns to `ð` when it exists between two vowels. This language has the following vowels in its inventory: `a,e,i,o,u,æ,å,ø`. You could write the rules like this[^5]:

| in | out | context_before | context_after |
|---|---|---|---|
| dd | ð | (a\|e\|i\|o\|u\|æ\|å\|ø) | (a\|e\|i\|o\|u\|æ\|å\|ø) |

But, if there are lots of rules with these vowels, this could get very tedious, not to mention annoying if the characters in the set change at some point. It is also less readable, and leaves the reader of the mapping to infer the meaning of the rule.

So, in a separate file, by convention it is usually called `abbreviations.csv`, you can define a list of sets where each row is a new set. The first column contains the name of the set (by convention this is capitalized), and you can add characters to every following column. So, for example:

|    variable name       |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | 
|-----------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| 
| VOWEL     | a | e | i | o | u | æ | å | ø |   |   |   |   |   |   |   | 
| CONSONANT | p | b | t | d | k | g | f | s | h | v | j | r | l | m | n | 
| FRONT     | i | e | œ | ø | y |   |   |   |   |   |   |   |   |   |   | 
| BACK      | u | o | a |   |   |   |   |   |   |   |   |   |   |   |   | 


Then, in your configuration, you can add the file to a specific mapping using `abbreviations: abbreviations.csv`. After adding it to your mapping, you can write the above rule like this instead:

| in | out | context_before | context_after |
|---|---|---|---|
| dd | ð | VOWEL | VOWEL |

You can also use abbreviations like this in the G2P studio by writing them in the section at the bottom of the page titled 'Custom Abbreviations' They will be automatically applied to your custom rules above.

{% picture g2p-abbs.png %}

### Regular Expressions

[Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) are used ubiquitously in programming to do define certain search patterns in text. In fact, this is how `g2p` rules work! They eventually get compiled into a regular expression too. For the most part, you can add regular expression syntax to your rules. So, suppose you wanted to write a rule that deleted word-final 's', you could write the following:

| in | out | context_before | context_after |
|---|---|---|---|
| s |  | \S | \s|$ |

As you can see [in this handy cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) our rule turns 's' into nothing if it is preceded by \S (not white space) and followed by either \s (white space) or $ (end of the text).

Note: There are some 'gotchas' with writing regular expressions using g2p. This is a technical note, but if you're writing some complicated regular expressions and they're not working, don't hesitate to [raise an issue](https://github.com/roedoejet/g2p/issues/new/choose). For example there are some active issues around edge cases where regular expressions and `g2p`'s custom [syntax for indices](#using-specific-indices) don't play nice together.

### Using specific indices

Even people familiar with using `g2p` might not be aware that one of its main features is that it preserves indices between input and output segments. Meaning that when we convert from something like 'kæt' to 'kʰæt' as in the first example, `g2p` knows that its the 'k' that turned into the 'k' and 'ʰ'.

{% picture k-to-kh.png %}

The default interpretation by `g2p` is that it zips up the characters between the input and the output of a rule until it reaches the end of either one, then it glombs on any remaining characters the last character of the shorter one (whether that's the input or output). For example, compare the following examples where 'abc' is converted to 'ab':

{% picture abc-ab.png %}

and where 'ab' is converted to 'abc':

{% picture ab-abc.png %}

But what if we want to show that it was actually 'a' that turned into 'c', and 'b' that turned into 'ab'? Well, we can use special `g2p` syntax for writing explicitly writing these indices. instead of, 

| in | out | context_before | context_after |
|---|---|---|---|
| ab | abc | |  |

we can write

| in | out | context_before | context_after |
|---|---|---|---|
| a{1}b{2} | ab{2}c{1} | |  |

Now, our indices will be properly preserved:

{% picture ab-abc-i.png %}

Basically, using the explicit indices syntax will break up your rule into a number of smaller rules that apply the same defaults of above but to explicit sets of characters. You must use curly brackets, but the choice of character you put inside is arbitrary - it just has to match on both sides. By convention, we use natural numbers. This will match all the characters to the left of each pair of curly brackets on the input with the matching index on the output. So here, 'a' is matched with 'c' and 'b' is matched with 'ab'.

These can get fairly complicated, so we recommend only using this functionality either for demonstration purposes, or for specific applications which require the preservation of indices.

## Applications

There are number of different software tools that are already making use of `g2p`. For general purpose use in Python, have a look at [this section](#advanced-use-in-python). G2P is also used extensively in the [ReadAlongs](#readalongs) project for creating interactive audio/text documents and in [Convertextract](#convertextract), a tool for going `g2p` transformations in Microsoft Office documents while preserving the original document formatting.

### ReadAlongs

ReadAlongs is an exploratory research project from the [National Research Council's Indigenous Language Technology Project](https://nrc.canada.ca/en/research-development/research-collaboration/programs/canadian-indigenous-languages-technology-project). The central goal of the project is to find a way to align audio and text for languages without requiring *any* training data.

Let me explain how this works. Language revitalization communities have potentially many different recordings of language and some associated text, but mobilizing these materials into something that is educationally useful can be time-consuming. It's also tough from the learner's perspective to try and follow along the text (maybe a Word Document) just by simply playing the audio. It's easy to get lost in the recording quickly, and from personal experience, trying to do that usually involves a lot of frustrating rewinding.

What if we could develop a tool that *automatically* figured out what parts of the audio file corresponded to what parts of the text? In Natural Language Processing, this is called *forced alignment*, and this is fundamentally what the ReadAlongs project does. But how does it do it? In part, by using a whole bunch of `g2p` mappings!

First of all, it's good to know that forced alignment is essentially a "solved problem" for languages with a lot of data. If you know Python, and have 20+ hours of manually aligned text and audio in your language, you can train a model using one of the few tools out there developed for this task, like [the Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/). But what if you don't have that much data (or any) and what if you don't know Python? If your language has a `g2p` mapping between its writing system and the ipa, you can likely circumvent that whole process. Here's how:

We take a model for doing forced alignment on English (that has a heck of a lot of data) and we manually write a `g2p` mapping from the writing system to IPA for the language we want to make a ReadAlong for. For example's sake let's say, Kanyen'kéha (Mohawk). This step requires somebody who is both familiar with the writing system used, and with the International Phonetic Alphabet. 

Then, we use the very cool Python library [PanPhon](https://github.com/dmort27/panphon) to figure out the mapping between the IPA characters in Mohawk and their *closest* IPA equivalents in English. We call this mapping a "Kanyen'kéha IPA to English IPA" mapping. If you have already created your mapping between the orthography and the IPA, you can generate this "Kanyen'kéha IPA to English IPA" using the `g2p` command line. After [installing](#installation) `g2p` and once you have made your mapping, update `g2p` by running `g2p update`. Then, you can run `g2p generate-mapping <input-code> --ipa`. So in our example for Kanyen'kéha (moh) we would run `g2p genearate-mapping moh --ipa`. That will automatically generate a mapping in `g2p/mappings/langs/generated` - do not edit this file or its configuration unless you really know what you're doing, because it will get overwritten. After generation, you can run `g2p update` and your mapping will be [usable](#usage) within `g2p`!

Then, we convert the ReadAlong in question from its original orthographic form all the way to English IPA (well, actually to 'Arpabet' which is an ASCII-compliant phonetic transcription standard that the alignment model understands).

Then, after we "do the alignment" (ie figure out which parts of the audio correspond to which part of the text), ReadAlongs puts it all back together again and ta-da! There is your aligned audio and text!

This is actually where the introductory picture to this blog post comes from! In the graphic below, you can see the word 'bonjour' in French converted to its IPA transcription. Then, its IPA transcription gets converted into its corresponding *English* IPA transcription (notice how uvular /ʁ/ gets transformed to alveolo-palatal /ʒ/). Then finally, the English IPA form is converted to Arpabet. So in all, we have an input of 'bonjour' that gets output as 'B AO N ZH UW ZH' and the aligner runs on that form.

{% picture bonjour-g2p.png %}

You can recreate this type of animation using the G2P studio by selecting the French (`fra`) to English Arpabet (`eng-arpabet`) mapping, choosing 'animate' and then typing your text in the input area as seen below:

{% picture g2p-animate.png %}

ReadAlongs exports to a variety of formats, including epub (for e-readers), [Praat](https://www.fon.hum.uva.nl/praat/) TextGrids, [ELAN](https://archive.mpi.nl/tla/elan) files, various subtitle formats, and an embeddable Web Component for your website. This project is still *very* unstable though, and it is **not** yet recommended to use it in production.

** INSERT ACTUAL READALONG HERE **

### Convertextract

Convertextract is a tool that lets you use `g2p` mappings to convert text inside a variety of different documents. Have a look at the [convertextract repo readme](https://github.com/roedoejet/convertextract) or at Fineen Davis' [blog post]({{ "convertextract-app" | absolute_url }}) about the Convertextract GUI.

[Convertextract]({{ "convertextract-app" | absolute_url }})

### Advanced: use in Python

#### Installation

We recommend installing `g2p` using [pip](https://en.wikipedia.org/wiki/Pip_(package_manager)):

```python
pip install g2p
```

If you are going to be creating your own mapping or editing `g2p` in any way, you must first fork and clone `g2p`:

```bash
git clone https://github.com/YourGitHubUsername/g2p.git
```

and then do an *editable* pip installation:

```bash
cd g2p && pip install -e .
```

#### Usage

Using `g2p` within Python can be done, either programatically using the `make_g2p` function:

```python
>>> from g2p import make_g2p
>>> transducer = make_g2p('dan', 'eng-arpabet')
>>> transducer('hej').output_string
'HH EH Y'
```

#### Command Line Interface

You can also use `g2p` from the command line. The basic command for conversions is:

```bash
g2p convert <input_text> <in_lang> <out_lang>
```

So in practice:

```bash
$ g2p convert hej dan eng-arpabet
HH EH YY
```

## Advanced: contributing your rules to the main `g2p` library

You've written some cool rules and you want to contribute, that's awesome! There are lots of benefits to contributing your mapping to `g2p`. First of all, once your mapping is accepted, you'll have it available and live on [G2P Studio](https://g2p-studio.herokuapp.com). Second, once the next version of `g2p` is release with your mapping, it will be automatically built in to the [Convertextract](#convertextract) library.

If your mapping is between a language's writing system and the IPA, you can also get ReadAlongs support.

So, you write your mapping once, and you get three things for free (G2P studio, convertextract and readalongs). Here's how:

1. Fork g2p
2. Add a folder for your language using the appropriate [ISO 639.3 code](https://en.wikipedia.org/wiki/List_of_ISO_639-3_codes) to `g2p/mappings/langs`
3. Add a config.yaml file as described [here](#mapping-configuration)
4. Add your mapping in that same folder (`g2p/mappings/langs/<yourlang>`)
5. Run `g2p update` to add your mapping to `g2p`
6. Add some test data to `g2p/tests/public/data`. 

  This is done by adding either a CSV/TSV/PSV file with 4 columns. The first is the input language code, the second is the output language code, the third is the input and the fourth is the asserted output of that mapping and input. Here is an example between French (fra) and French IPA (fra-ipa) asserting that 'manger' results in 'mɑ̃ʒe':

  ```psv
  fra|fra-ipa|manger|mɑ̃ʒe
  fra|fra-ipa|écoutons|ekutɔ̃
  ```

7. Submit your changes by creating a [pull request](https://github.com/roedoejet/g2p/compare)

That's it! Either myself, or somebody else will review the changes, and you will get credit for those mappings and be added to the [list of contributors](https://github.com/roedoejet/g2p/graphs/contributors)

## Advanced: adding a 'pre-processing' mapping

...more to come...

### Footnotes

[^1]: Because the word 'letter' usually refers to a character within a specific alphabet or writing system, instead of 'letter', I'm going to use the word 'character' throughout this post.
[^2]: A Python 'library' is a collection of code
[^3]: Orthographic characters are circumfixed with apostrophes like 'a', broad IPA typically uses forward slashes like /k/ and narrow IPA typically uses square brackets like [kʰ]
[^4]: I'm using the free, hobby plan at <https://www.heroku.com/> to host it though, so occasionally the server goes to sleep. If you first go to the site and it takes a few seconds to boot up, don't worry!
[^5]: You'll notice that the syntax here is a little weird, what the heck are all of those pipes (the up-down things like \|) doing there? That's because I'm using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to express a **OR** e **OR** u etc... For more info, check out the [section on regular expressions](#regular-expressions)